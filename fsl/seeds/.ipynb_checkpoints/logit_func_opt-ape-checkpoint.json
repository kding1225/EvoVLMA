[
     {
          "algorithm": "The algorithm sums up two logits: the logits generated by zero-shot classifier and the logits generated by a cache model. The first logits are computed by applying linear transformation to test features. The second logits are obtained by first computing the similarity matrix between test and train features and then multiplying the transformed similarity matrix to soft train label matrix. While computing image-image similarity, selected feature channels are used.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n\n    # feature selection\n    train_feats = train_feats.view(-1, train_feats.shape[-1])\n    new_clip_weights = F.normalize(clip_weights[:, indices], dim=-1)\n    new_train_feats = F.normalize(train_feats[:, indices], dim=-1)\n    new_test_feats = F.normalize(test_feats[:, indices], dim=-1)\n\n    # compute cache logits\n    train_labels = F.one_hot(train_labels.view(-1))\n    key_logits = (new_train_feats @ new_clip_weights.t()).softmax(1)\n    cache_div = torch.sum(train_labels * torch.log2((train_labels + 1e-6) / (key_logits + 1e-6)), dim=1)[:, None]\n    soft_train_labels = train_labels * (cache_div * alpha2).exp()\n    R_fF = new_test_feats @ new_train_feats.t()\n    cache_logits = ((-1) * (alpha1 - alpha1 * R_fF)).exp() @ soft_train_labels\n\n    # zero-shot logits\n    clip_logits = 100*test_feats @ clip_weights.t()\n\n    # fused logits\n    logits = clip_logits + alpha0 * cache_logits\n    \n    return logits",
          "objective": 10000.0,
          "other_inf": null
     }
]
