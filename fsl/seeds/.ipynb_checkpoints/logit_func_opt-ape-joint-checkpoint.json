[
     {
          "algorithm": "The algorithm has two steps. First, select important feature channels according to a devised criterion. The criterion gives high scores for channels that minimize the inter-class similarity of the concatenated features of visual and category textual features, but maximize the variance of category textual features. Second, compute logits by combining the logits generated by CLIP's zero-shot classifier and the logits generated by a cache model. The first logits are computed by applying linear transformation to test features. The second logits are obtained by first computing the similarity matrix between test and train features and then multiplying the transformed similarity matrix to soft train label matrix. While computing image-image similarity, selected feature channels are used.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef feat_selection(clip_weights, train_feats, w0, w1, topk):\n    # feature selection\n    feats = torch.cat([clip_weights.unsqueeze(1), train_feats], dim=1)\n    cate_num, samp_num, feat_dim = feats.shape\n    sim_sum = torch.zeros((feat_dim)).cuda()\n    count = 0\n    for i in range(cate_num):\n        for j in range(cate_num):\n            if i != j:\n                sim_sum += (feats[i].unsqueeze(1) * feats[j].unsqueeze(0)).mean(dim=0).mean(dim=0)\n                count += samp_num*samp_num\n    sim = sim_sum / count\n    criterion = (-1) * w0 * sim + w1 * torch.var(clip_weights, dim=0)\n    _, indices = torch.topk(criterion, k=topk)\n    return indices\n\ndef compute_logits_with_fs(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n\n    train_feats = train_feats.view(-1, train_feats.shape[-1])\n    new_clip_weights = F.normalize(clip_weights[:, indices], dim=-1)\n    new_train_feats = F.normalize(train_feats[:, indices], dim=-1)\n    new_test_feats = F.normalize(test_feats[:, indices], dim=-1)\n\n    # compute cache logits\n    train_labels = F.one_hot(train_labels.view(-1))\n    key_logits = (new_train_feats @ new_clip_weights.t()).softmax(1)\n    cache_div = torch.sum(train_labels * torch.log2((train_labels + 1e-6) / (key_logits + 1e-6)), dim=1)[:, None]\n    soft_train_labels = train_labels * (cache_div * alpha2).exp()\n    R_fF = new_test_feats @ new_train_feats.t()\n    cache_logits = ((-1) * (alpha1 - alpha1 * R_fF)).exp() @ soft_train_labels\n\n    # zero-shot logits\n    clip_logits = 100*test_feats @ clip_weights.t()\n\n    # fused logits\n    logits = clip_logits + alpha0 * cache_logits\n    \n    return logits\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, w0, w1, topk, alpha0, alpha1, alpha2):\n\n    # feature selection\n    indices = feat_selection(clip_weights, train_feats, w0, w1, topk)\n    \n    # compute logits\n    logits = compute_logits_with_fs(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2)\n    \n    return logits",
          "objective": 10000.0,
          "other_inf": null
     }
]
