[
     {
          "algorithm": "The new algorithm selects feature channels by maximizing the class-specific feature importance weighted by the inter-class divergence, while minimizing the intra-class variance and the correlation between selected channels, weighted by hyper-parameters.",
          "code": "import torch\n\ndef feat_selection(clip_weights, train_feats, w0, w1, topk):\n    cate_num, samp_num, feat_dim = train_feats.shape\n    \n    # Compute class-specific feature importance\n    class_importance = torch.einsum('cd,cnd->d', clip_weights, train_feats) / (cate_num * samp_num)\n    \n    # Compute inter-class divergence\n    class_means = train_feats.mean(dim=1)  # Shape: (cate_num, feat_dim)\n    inter_div = torch.var(class_means, dim=0)  # Shape: (feat_dim,)\n    \n    # Compute intra-class variance\n    intra_var = torch.var(train_feats, dim=(0, 1))  # Shape: (feat_dim,)\n    \n    # Compute correlation between feature channels\n    correlation = torch.einsum('cnd,cnd->d', train_feats, train_feats) / (cate_num * samp_num)\n    \n    # Combine criteria\n    criterion = w0 * (class_importance * inter_div) - w1 * (intra_var + correlation)\n    \n    # Select topk features\n    _, indices = torch.topk(criterion, k=topk)\n    return indices",
          "objective": 39.9682,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm selects feature channels by maximizing the inter-class divergence of visual features while minimizing the correlation between selected channels, weighted by hyper-parameters.",
          "code": "import torch\n\ndef feat_selection(clip_weights, train_feats, w0, w1, topk):\n    cate_num, samp_num, feat_dim = train_feats.shape\n    \n    # Compute inter-class divergence\n    class_means = train_feats.mean(dim=1)  # Shape: (cate_num, feat_dim)\n    inter_div = torch.var(class_means, dim=0)  # Shape: (feat_dim,)\n    \n    # Compute correlation between feature channels\n    correlation = torch.einsum('cnd,cnd->d', train_feats, train_feats) / (cate_num * samp_num)\n    \n    # Combine criteria\n    criterion = w0 * inter_div - w1 * correlation\n    \n    # Select topk features\n    _, indices = torch.topk(criterion, k=topk)\n    return indices",
          "objective": 39.99408,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm selects feature channels by maximizing the class-discriminative power of features, measured by the ratio of inter-class variance to intra-class variance, while minimizing the mutual information between selected channels, weighted by hyper-parameters.",
          "code": "import torch\n\ndef feat_selection(clip_weights, train_feats, w0, w1, topk):\n    cate_num, samp_num, feat_dim = train_feats.shape\n    \n    # Compute inter-class variance\n    class_means = train_feats.mean(dim=1)  # Shape: (cate_num, feat_dim)\n    inter_var = torch.var(class_means, dim=0)  # Shape: (feat_dim,)\n    \n    # Compute intra-class variance\n    intra_var = torch.var(train_feats, dim=(0, 1))  # Shape: (feat_dim,)\n    \n    # Compute class-discriminative power\n    disc_power = inter_var / (intra_var + 1e-8)\n    \n    # Compute mutual information between feature channels\n    cov_matrix = torch.einsum('cnd,cnd->d', train_feats, train_feats) / (cate_num * samp_num)\n    mutual_info = torch.log(cov_matrix + 1e-8)\n    \n    # Combine criteria\n    criterion = w0 * disc_power - w1 * mutual_info\n    \n    # Select topk features\n    _, indices = torch.topk(criterion, k=topk)\n    return indices",
          "objective": 40.02948,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm selects feature channels by maximizing the class-specific feature relevance weighted by the intra-class compactness, while minimizing the cross-class similarity and the feature redundancy between selected channels, weighted by hyper-parameters.",
          "code": "import torch\n\ndef feat_selection(clip_weights, train_feats, w0, w1, topk):\n    cate_num, samp_num, feat_dim = train_feats.shape\n    \n    # Compute class-specific feature relevance\n    class_relevance = torch.einsum('cd,cnd->d', clip_weights, train_feats) / (cate_num * samp_num)\n    \n    # Compute intra-class compactness\n    class_means = train_feats.mean(dim=1)  # Shape: (cate_num, feat_dim)\n    intra_compactness = torch.norm(train_feats - class_means.unsqueeze(1), p=2, dim=(0, 1)) / (cate_num * samp_num)\n    \n    # Compute cross-class similarity\n    cross_similarity = torch.einsum('cd,cd->d', class_means, class_means) / (cate_num * cate_num)\n    \n    # Compute feature redundancy\n    feature_redundancy = torch.einsum('cnd,cnd->d', train_feats, train_feats) / (cate_num * samp_num)\n    \n    # Combine criteria\n    criterion = w0 * (class_relevance * intra_compactness) - w1 * (cross_similarity + feature_redundancy)\n    \n    # Select topk features\n    _, indices = torch.topk(criterion, k=topk)\n    return indices",
          "objective": 40.06007,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm selects feature channels by maximizing the class-specific feature importance weighted by the inter-class divergence, while minimizing the redundancy between selected channels, weighted by hyper-parameters.",
          "code": "import torch\n\ndef feat_selection(clip_weights, train_feats, w0, w1, topk):\n    cate_num, samp_num, feat_dim = train_feats.shape\n    \n    # Compute class-specific feature importance\n    class_importance = torch.einsum('cd,cnd->d', clip_weights, train_feats) / (cate_num * samp_num)\n    \n    # Compute inter-class divergence\n    class_means = train_feats.mean(dim=1)  # Shape: (cate_num, feat_dim)\n    inter_div = torch.var(class_means, dim=0)  # Shape: (feat_dim,)\n    \n    # Compute redundancy between feature channels\n    redundancy = torch.einsum('cnd,cnd->d', train_feats, train_feats) / (cate_num * samp_num)\n    \n    # Combine criteria\n    criterion = w0 * (class_importance * inter_div) - w1 * redundancy\n    \n    # Select topk features\n    _, indices = torch.topk(criterion, k=topk)\n    return indices",
          "objective": 40.11912,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm selects feature channels by maximizing the discriminative power of visual features through class-specific feature importance, while minimizing the redundancy between selected channels, weighted by hyper-parameters.",
          "code": "import torch\n\ndef feat_selection(clip_weights, train_feats, w0, w1, topk):\n    cate_num, samp_num, feat_dim = train_feats.shape\n    \n    # Compute class-specific feature importance\n    class_importance = torch.einsum('cd,cnd->d', clip_weights, train_feats) / (cate_num * samp_num)\n    \n    # Compute redundancy between feature channels\n    redundancy = torch.einsum('cnd,cnd->d', train_feats, train_feats) / (cate_num * samp_num)\n    \n    # Combine criteria\n    criterion = w0 * class_importance - w1 * redundancy\n    \n    # Select topk features\n    _, indices = torch.topk(criterion, k=topk)\n    return indices",
          "objective": 40.1451,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm selects feature channels by maximizing the class-specific feature relevance weighted by the intra-class compactness, while minimizing the cross-class similarity between selected channels, weighted by hyper-parameters.",
          "code": "import torch\n\ndef feat_selection(clip_weights, train_feats, w0, w1, topk):\n    cate_num, samp_num, feat_dim = train_feats.shape\n    \n    # Compute class-specific feature relevance\n    class_relevance = torch.einsum('cd,cnd->d', clip_weights, train_feats) / (cate_num * samp_num)\n    \n    # Compute intra-class compactness\n    class_means = train_feats.mean(dim=1)  # Shape: (cate_num, feat_dim)\n    intra_compactness = torch.norm(train_feats - class_means.unsqueeze(1), p=2, dim=(0, 1)) / (cate_num * samp_num)\n    \n    # Compute cross-class similarity\n    cross_similarity = torch.einsum('cd,cd->d', class_means, class_means) / (cate_num * cate_num)\n    \n    # Combine criteria\n    criterion = w0 * (class_relevance * intra_compactness) - w1 * cross_similarity\n    \n    # Select topk features\n    _, indices = torch.topk(criterion, k=topk)\n    return indices",
          "objective": 40.20109,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm selects feature channels by maximizing the class-specific feature importance weighted by the inter-class divergence and intra-class consistency, while minimizing the redundancy between selected channels, weighted by hyper-parameters, and incorporating a novel feature diversity measure based on the pairwise cosine similarity between feature channels.",
          "code": "import torch\n\ndef feat_selection(clip_weights, train_feats, w0, w1, topk):\n    cate_num, samp_num, feat_dim = train_feats.shape\n    \n    # Compute class-specific feature importance\n    class_importance = torch.einsum('cd,cnd->d', clip_weights, train_feats) / (cate_num * samp_num)\n    \n    # Compute inter-class divergence\n    class_means = train_feats.mean(dim=1)  # Shape: (cate_num, feat_dim)\n    inter_div = torch.var(class_means, dim=0)  # Shape: (feat_dim,)\n    \n    # Compute intra-class consistency\n    intra_consistency = torch.mean(torch.var(train_feats, dim=1), dim=0)  # Shape: (feat_dim,)\n    \n    # Compute redundancy between feature channels\n    redundancy = torch.einsum('cnd,cnd->d', train_feats, train_feats) / (cate_num * samp_num)\n    \n    # Compute feature diversity based on pairwise cosine similarity\n    feature_norms = torch.norm(train_feats, p=2, dim=2)  # Shape: (cate_num, samp_num)\n    normalized_feats = train_feats / feature_norms.unsqueeze(2)  # Shape: (cate_num, samp_num, feat_dim)\n    pairwise_cosine = torch.einsum('cnd,cnd->d', normalized_feats, normalized_feats) / (cate_num * samp_num)\n    diversity = 1 - pairwise_cosine\n    \n    # Combine criteria\n    criterion = w0 * (class_importance * inter_div * intra_consistency) - w1 * redundancy + diversity\n    \n    # Select topk features\n    _, indices = torch.topk(criterion, k=topk)\n    return indices",
          "objective": 40.20945,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm selects feature channels by maximizing the mutual information between the visual features and the textual features, while also considering the sparsity of the feature channels to ensure diversity.",
          "code": "import torch\n\ndef feat_selection(clip_weights, train_feats, w0, w1, topk):\n    cate_num, samp_num, feat_dim = train_feats.shape\n    \n    # Compute mutual information between visual and textual features\n    visual_mean = train_feats.mean(dim=1)  # (c, d)\n    correlation = torch.matmul(visual_mean, clip_weights.T)  # (c, c)\n    mutual_info = torch.diag(correlation)  # (c,)\n    \n    # Compute sparsity of feature channels\n    sparsity = torch.norm(train_feats, p=1, dim=(0, 1)) / (cate_num * samp_num)\n    \n    # Combine criteria\n    criterion = w0 * mutual_info.unsqueeze(1).expand(-1, feat_dim).mean(dim=0) - w1 * sparsity\n    \n    # Select topk features\n    _, indices = torch.topk(criterion, k=topk)\n    \n    return indices",
          "objective": 40.21314,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm selects feature channels by maximizing the class-specific feature importance weighted by the inter-class divergence and the class-specific feature variance, while minimizing the intra-class variance and the correlation between selected channels, weighted by hyper-parameters.",
          "code": "import torch\n\ndef feat_selection(clip_weights, train_feats, w0, w1, topk):\n    cate_num, samp_num, feat_dim = train_feats.shape\n    \n    # Compute class-specific feature importance\n    class_importance = torch.einsum('cd,cnd->d', clip_weights, train_feats) / (cate_num * samp_num)\n    \n    # Compute inter-class divergence\n    class_means = train_feats.mean(dim=1)  # Shape: (cate_num, feat_dim)\n    inter_div = torch.var(class_means, dim=0)  # Shape: (feat_dim,)\n    \n    # Compute class-specific feature variance\n    class_var = torch.var(train_feats, dim=1).mean(dim=0)  # Shape: (feat_dim,)\n    \n    # Compute intra-class variance\n    intra_var = torch.var(train_feats, dim=(0, 1))  # Shape: (feat_dim,)\n    \n    # Compute correlation between feature channels\n    correlation = torch.einsum('cnd,cnd->d', train_feats, train_feats) / (cate_num * samp_num)\n    \n    # Combine criteria\n    criterion = w0 * (class_importance * inter_div * class_var) - w1 * (intra_var + correlation)\n    \n    # Select topk features\n    _, indices = torch.topk(criterion, k=topk)\n    return indices",
          "objective": 40.22067,
          "other_inf": null
     }
]