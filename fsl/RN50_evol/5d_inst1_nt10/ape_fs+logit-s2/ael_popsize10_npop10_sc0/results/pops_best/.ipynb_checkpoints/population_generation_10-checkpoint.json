[
     {
          "algorithm": "The new algorithm computes classification logits by combining zero-shot logits, a feature-channel importance-weighted class-specific feature aggregation, a feature-channel importance-weighted cache model, and a feature-channel importance-weighted regularization term, with an additional feature-channel importance-weighted similarity term to enhance discriminative power, while introducing a novel feature-channel importance-weighted class-specific feature aggregation term.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    # feature selection\n    train_feats = train_feats.view(-1, train_feats.shape[-1])\n    new_clip_weights = F.normalize(clip_weights[:, indices], dim=-1)\n    new_train_feats = F.normalize(train_feats[:, indices], dim=-1)\n    new_test_feats = F.normalize(test_feats[:, indices], dim=-1)\n\n    # compute zero-shot logits\n    clip_logits = 100 * test_feats @ clip_weights.t()\n\n    # compute class-specific feature aggregation with feature channel importance\n    class_feats = torch.stack([new_train_feats[train_labels.view(-1) == i].mean(dim=0) for i in range(clip_weights.shape[0])])\n    class_feats = F.normalize(class_feats, dim=-1)\n    class_sim = new_test_feats @ class_feats.t()\n    weighted_class_sim = class_sim * (1 + alpha1 * torch.sum(new_test_feats ** 2, dim=1, keepdim=True))\n\n    # compute cache logits with feature channel importance\n    train_labels = F.one_hot(train_labels.view(-1))\n    key_logits = (new_train_feats @ new_clip_weights.t()).softmax(1)\n    cache_div = torch.sum(train_labels * torch.log2((train_labels + 1e-6) / (key_logits + 1e-6)), dim=1)[:, None]\n    soft_train_labels = train_labels * (cache_div * alpha2).exp()\n    R_fF = new_test_feats @ new_train_feats.t()\n    cache_logits = ((-1) * (alpha1 - alpha1 * R_fF)).exp() @ soft_train_labels\n\n    # compute feature-channel importance-weighted regularization term\n    importance_weights = torch.zeros_like(test_feats)\n    importance_weights[:, indices] = 1.0\n    weighted_test_feats = test_feats * importance_weights\n    weighted_logits = 100 * weighted_test_feats @ clip_weights.t()\n\n    # compute feature-channel importance-weighted similarity term\n    similarity_term = new_test_feats @ new_clip_weights.t()\n\n    # compute novel feature-channel importance-weighted class-specific feature aggregation term\n    novel_class_sim = new_test_feats @ class_feats.t() * (1 + alpha0 * torch.sum(new_test_feats ** 2, dim=1, keepdim=True))\n\n    # fused logits with regularization and similarity term\n    logits = clip_logits + alpha0 * weighted_class_sim + alpha1 * cache_logits + alpha2 * weighted_logits + alpha0 * similarity_term + alpha1 * novel_class_sim\n    \n    return logits",
          "objective": 38.27041,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes logits by combining zero-shot classification scores, a class-specific feature aggregation with feature-channel importance, and a feature-channel importance-weighted cache model, while introducing a novel feature-channel importance-based weighting mechanism for the zero-shot logits and cache logits.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    # feature selection\n    train_feats = train_feats.view(-1, train_feats.shape[-1])\n    new_clip_weights = F.normalize(clip_weights[:, indices], dim=-1)\n    new_train_feats = F.normalize(train_feats[:, indices], dim=-1)\n    new_test_feats = F.normalize(test_feats[:, indices], dim=-1)\n\n    # compute zero-shot logits with feature channel importance\n    clip_logits = 100 * (test_feats @ clip_weights.t()) * (1 + alpha0 * torch.sum(test_feats[:, indices] ** 2, dim=1, keepdim=True))\n\n    # compute class-specific feature aggregation with feature channel importance\n    class_feats = torch.stack([new_train_feats[train_labels.view(-1) == i].mean(dim=0) for i in range(clip_weights.shape[0])])\n    class_feats = F.normalize(class_feats, dim=-1)\n    class_sim = new_test_feats @ class_feats.t()\n    weighted_class_sim = class_sim * (1 + alpha1 * torch.sum(new_test_feats ** 2, dim=1, keepdim=True))\n\n    # compute cache logits with feature channel importance-based weighting\n    train_labels = F.one_hot(train_labels.view(-1))\n    key_logits = (new_train_feats @ new_clip_weights.t()).softmax(1)\n    cache_div = torch.sum(train_labels * torch.log2((train_labels + 1e-6) / (key_logits + 1e-6)), dim=1)[:, None]\n    soft_train_labels = train_labels * (cache_div * alpha2).exp()\n    R_fF = new_test_feats @ new_train_feats.t()\n    cache_logits = ((-1) * (alpha1 - alpha1 * R_fF)).exp() @ soft_train_labels\n    cache_logits = cache_logits * (1 + alpha0 * torch.sum(new_test_feats ** 2, dim=1, keepdim=True))\n\n    # fused logits with feature channel importance\n    logits = clip_logits + alpha0 * weighted_class_sim + alpha2 * cache_logits\n    \n    return logits",
          "objective": 38.48887,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes classification logits by combining zero-shot logits, a feature-channel importance-weighted class-specific feature aggregation, a feature-channel importance-weighted cache model, and a feature-channel importance-weighted regularization term, with an additional feature-channel importance-weighted similarity term to enhance discriminative power.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    # feature selection\n    train_feats = train_feats.view(-1, train_feats.shape[-1])\n    new_clip_weights = F.normalize(clip_weights[:, indices], dim=-1)\n    new_train_feats = F.normalize(train_feats[:, indices], dim=-1)\n    new_test_feats = F.normalize(test_feats[:, indices], dim=-1)\n\n    # compute zero-shot logits\n    clip_logits = 100 * test_feats @ clip_weights.t()\n\n    # compute class-specific feature aggregation with feature channel importance\n    class_feats = torch.stack([new_train_feats[train_labels.view(-1) == i].mean(dim=0) for i in range(clip_weights.shape[0])])\n    class_feats = F.normalize(class_feats, dim=-1)\n    class_sim = new_test_feats @ class_feats.t()\n    weighted_class_sim = class_sim * (1 + alpha1 * torch.sum(new_test_feats ** 2, dim=1, keepdim=True))\n\n    # compute cache logits with feature channel importance\n    train_labels = F.one_hot(train_labels.view(-1))\n    key_logits = (new_train_feats @ new_clip_weights.t()).softmax(1)\n    cache_div = torch.sum(train_labels * torch.log2((train_labels + 1e-6) / (key_logits + 1e-6)), dim=1)[:, None]\n    soft_train_labels = train_labels * (cache_div * alpha2).exp()\n    R_fF = new_test_feats @ new_train_feats.t()\n    cache_logits = ((-1) * (alpha1 - alpha1 * R_fF)).exp() @ soft_train_labels\n\n    # compute feature-channel importance-weighted regularization term\n    importance_weights = torch.zeros_like(test_feats)\n    importance_weights[:, indices] = 1.0\n    weighted_test_feats = test_feats * importance_weights\n    weighted_logits = 100 * weighted_test_feats @ clip_weights.t()\n\n    # compute feature-channel importance-weighted similarity term\n    similarity_term = new_test_feats @ new_clip_weights.t()\n\n    # fused logits with regularization and similarity term\n    logits = clip_logits + alpha0 * weighted_class_sim + alpha1 * cache_logits + alpha2 * weighted_logits + alpha0 * similarity_term\n    \n    return logits",
          "objective": 38.58179,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes logits by combining zero-shot classification scores, a class-specific feature aggregation with feature-channel importance, and a feature-channel importance-weighted cache model, while introducing a novel feature-channel importance-based weighting mechanism for the cache logits.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    # feature selection\n    train_feats = train_feats.view(-1, train_feats.shape[-1])\n    new_clip_weights = F.normalize(clip_weights[:, indices], dim=-1)\n    new_train_feats = F.normalize(train_feats[:, indices], dim=-1)\n    new_test_feats = F.normalize(test_feats[:, indices], dim=-1)\n\n    # compute zero-shot logits\n    clip_logits = 100 * test_feats @ clip_weights.t()\n\n    # compute class-specific feature aggregation with feature channel importance\n    class_feats = torch.stack([new_train_feats[train_labels.view(-1) == i].mean(dim=0) for i in range(clip_weights.shape[0])])\n    class_feats = F.normalize(class_feats, dim=-1)\n    class_sim = new_test_feats @ class_feats.t()\n    weighted_class_sim = class_sim * (1 + alpha1 * torch.sum(new_test_feats ** 2, dim=1, keepdim=True))\n\n    # compute cache logits with feature channel importance-based weighting\n    train_labels = F.one_hot(train_labels.view(-1))\n    key_logits = (new_train_feats @ new_clip_weights.t()).softmax(1)\n    cache_div = torch.sum(train_labels * torch.log2((train_labels + 1e-6) / (key_logits + 1e-6)), dim=1)[:, None]\n    soft_train_labels = train_labels * (cache_div * alpha2).exp()\n    R_fF = new_test_feats @ new_train_feats.t()\n    cache_logits = ((-1) * (alpha1 - alpha1 * R_fF)).exp() @ soft_train_labels\n    cache_logits = cache_logits * (1 + alpha0 * torch.sum(new_test_feats ** 2, dim=1, keepdim=True))\n\n    # fused logits with feature channel importance\n    logits = clip_logits + alpha0 * weighted_class_sim + alpha2 * cache_logits\n    \n    return logits",
          "objective": 38.69594,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes classification logits by combining zero-shot logits, a feature-channel importance-weighted similarity-based cache logits, a class-specific feature aggregation, and a feature-channel importance-weighted zero-shot logits, with an additional feature-channel importance-weighted regularization term to enhance discriminative power.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    # feature selection\n    train_feats = train_feats.view(-1, train_feats.shape[-1])\n    new_clip_weights = F.normalize(clip_weights[:, indices], dim=-1)\n    new_train_feats = F.normalize(train_feats[:, indices], dim=-1)\n    new_test_feats = F.normalize(test_feats[:, indices], dim=-1)\n\n    # zero-shot logits\n    clip_logits = 100 * (test_feats @ clip_weights.t())\n\n    # compute class-specific feature aggregation\n    class_feats = torch.stack([new_train_feats[train_labels.view(-1) == i].mean(dim=0) for i in range(clip_weights.shape[0])])\n    class_feats = F.normalize(class_feats, dim=-1)\n    class_sim = new_test_feats @ class_feats.t()\n\n    # compute cache logits with feature channel importance\n    train_labels = F.one_hot(train_labels.view(-1))\n    key_logits = (new_train_feats @ new_clip_weights.t()).softmax(1)\n    cache_div = torch.sum(train_labels * torch.log2((train_labels + 1e-6) / (key_logits + 1e-6)), dim=1)[:, None]\n    soft_train_labels = train_labels * (cache_div * alpha2).exp()\n    R_fF = new_test_feats @ new_train_feats.t()\n    cache_logits = ((-1) * (alpha1 - alpha1 * R_fF)).exp() @ soft_train_labels\n\n    # feature-channel importance-weighted zero-shot logits\n    importance_weights = torch.zeros_like(test_feats)\n    importance_weights[:, indices] = 1.0\n    weighted_test_feats = test_feats * importance_weights\n    weighted_logits = 100 * weighted_test_feats @ clip_weights.t()\n\n    # feature-channel importance-weighted regularization term\n    reg_term = torch.norm(test_feats[:, indices], p=2, dim=1, keepdim=True) * alpha0\n\n    # fused logits\n    logits = clip_logits + alpha0 * cache_logits + alpha1 * class_sim + alpha2 * weighted_logits + reg_term\n    \n    return logits",
          "objective": 38.6968,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes classification logits by combining zero-shot logits, a feature-channel importance-weighted similarity-based cache logits, a class-specific feature aggregation, and a feature-channel importance-weighted zero-shot logits, leveraging the provided important feature channels for enhanced discriminative power.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    # feature selection\n    train_feats = train_feats.view(-1, train_feats.shape[-1])\n    new_clip_weights = F.normalize(clip_weights[:, indices], dim=-1)\n    new_train_feats = F.normalize(train_feats[:, indices], dim=-1)\n    new_test_feats = F.normalize(test_feats[:, indices], dim=-1)\n\n    # zero-shot logits\n    clip_logits = 100 * (test_feats @ clip_weights.t())\n\n    # compute class-specific feature aggregation\n    class_feats = torch.stack([new_train_feats[train_labels.view(-1) == i].mean(dim=0) for i in range(clip_weights.shape[0])])\n    class_feats = F.normalize(class_feats, dim=-1)\n    class_sim = new_test_feats @ class_feats.t()\n\n    # compute cache logits with feature channel importance\n    train_labels = F.one_hot(train_labels.view(-1))\n    key_logits = (new_train_feats @ new_clip_weights.t()).softmax(1)\n    cache_div = torch.sum(train_labels * torch.log2((train_labels + 1e-6) / (key_logits + 1e-6)), dim=1)[:, None]\n    soft_train_labels = train_labels * (cache_div * alpha2).exp()\n    R_fF = new_test_feats @ new_train_feats.t()\n    cache_logits = ((-1) * (alpha1 - alpha1 * R_fF)).exp() @ soft_train_labels\n\n    # feature-channel importance-weighted zero-shot logits\n    importance_weights = torch.zeros_like(test_feats)\n    importance_weights[:, indices] = 1.0\n    weighted_test_feats = test_feats * importance_weights\n    weighted_logits = 100 * weighted_test_feats @ clip_weights.t()\n\n    # fused logits\n    logits = clip_logits + alpha0 * cache_logits + alpha1 * class_sim + alpha2 * weighted_logits\n    \n    return logits",
          "objective": 38.6988,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes classification logits by combining zero-shot logits, a feature-channel importance-weighted similarity-based cache model, and a class-specific feature aggregation, with an additional feature-channel importance-weighted regularization term to enhance discriminative power.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    # feature selection\n    train_feats = train_feats.view(-1, train_feats.shape[-1])\n    new_clip_weights = F.normalize(clip_weights[:, indices], dim=-1)\n    new_train_feats = F.normalize(train_feats[:, indices], dim=-1)\n    new_test_feats = F.normalize(test_feats[:, indices], dim=-1)\n\n    # compute zero-shot logits\n    clip_logits = 100 * test_feats @ clip_weights.t()\n\n    # compute class-specific feature aggregation\n    class_feats = torch.stack([new_train_feats[train_labels.view(-1) == i].mean(dim=0) for i in range(clip_weights.shape[0])])\n    class_feats = F.normalize(class_feats, dim=-1)\n    class_sim = new_test_feats @ class_feats.t()\n\n    # compute feature-channel importance-weighted cache logits\n    train_labels = F.one_hot(train_labels.view(-1))\n    key_logits = (new_train_feats @ new_clip_weights.t()).softmax(1)\n    cache_div = torch.sum(train_labels * torch.log2((train_labels + 1e-6) / (key_logits + 1e-6)), dim=1)[:, None]\n    soft_train_labels = train_labels * (cache_div * alpha2).exp()\n    R_fF = new_test_feats @ new_train_feats.t()\n    cache_logits = ((-1) * (alpha1 - alpha1 * R_fF)).exp() @ soft_train_labels\n\n    # compute feature-channel importance-weighted regularization term\n    importance_weights = torch.zeros_like(test_feats)\n    importance_weights[:, indices] = 1.0\n    weighted_test_feats = test_feats * importance_weights\n    weighted_logits = 100 * weighted_test_feats @ clip_weights.t()\n\n    # fused logits with regularization\n    logits = clip_logits + alpha0 * cache_logits + alpha1 * class_sim + alpha2 * weighted_logits\n    \n    return logits",
          "objective": 38.70619,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes logits by combining zero-shot classification scores, a class-specific feature aggregation with feature-channel importance, and a feature-channel importance-weighted cache model, while introducing a feature-channel importance-based reweighting mechanism for the zero-shot logits and a feature-channel importance-based reweighting mechanism for the cache model.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    # feature selection\n    train_feats = train_feats.view(-1, train_feats.shape[-1])\n    new_clip_weights = F.normalize(clip_weights[:, indices], dim=-1)\n    new_train_feats = F.normalize(train_feats[:, indices], dim=-1)\n    new_test_feats = F.normalize(test_feats[:, indices], dim=-1)\n\n    # compute zero-shot logits with feature-channel importance reweighting\n    clip_logits = 100 * test_feats @ clip_weights.t()\n    channel_importance = torch.sum(new_test_feats ** 2, dim=1, keepdim=True)\n    clip_logits = clip_logits * (1 + alpha0 * channel_importance)\n\n    # compute class-specific feature aggregation with feature channel importance\n    class_feats = torch.stack([new_train_feats[train_labels.view(-1) == i].mean(dim=0) for i in range(clip_weights.shape[0])])\n    class_feats = F.normalize(class_feats, dim=-1)\n    class_sim = new_test_feats @ class_feats.t()\n    weighted_class_sim = class_sim * (1 + alpha1 * channel_importance)\n\n    # compute cache logits with feature channel importance reweighting\n    train_labels = F.one_hot(train_labels.view(-1))\n    key_logits = (new_train_feats @ new_clip_weights.t()).softmax(1)\n    cache_div = torch.sum(train_labels * torch.log2((train_labels + 1e-6) / (key_logits + 1e-6)), dim=1)[:, None]\n    soft_train_labels = train_labels * (cache_div * alpha2).exp()\n    R_fF = new_test_feats @ new_train_feats.t()\n    cache_logits = ((-1) * (alpha1 - alpha1 * R_fF)).exp() @ soft_train_labels\n    cache_logits = cache_logits * (1 + alpha2 * channel_importance)\n\n    # fused logits with feature channel importance\n    logits = clip_logits + alpha1 * weighted_class_sim + alpha2 * cache_logits\n    \n    return logits",
          "objective": 38.71164,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes logits by combining zero-shot classification scores, a class-specific feature aggregation with feature-channel importance, and a feature-channel importance-weighted cache model, leveraging selected feature channels for enhanced discriminative power, while introducing a feature-channel importance-based weighting mechanism for the zero-shot logits.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    # feature selection\n    train_feats = train_feats.view(-1, train_feats.shape[-1])\n    new_clip_weights = F.normalize(clip_weights[:, indices], dim=-1)\n    new_train_feats = F.normalize(train_feats[:, indices], dim=-1)\n    new_test_feats = F.normalize(test_feats[:, indices], dim=-1)\n\n    # compute zero-shot logits with feature channel importance\n    clip_logits = 100 * (test_feats @ clip_weights.t()) * (1 + alpha0 * torch.sum(test_feats[:, indices] ** 2, dim=1, keepdim=True))\n\n    # compute class-specific feature aggregation with feature channel importance\n    class_feats = torch.stack([new_train_feats[train_labels.view(-1) == i].mean(dim=0) for i in range(clip_weights.shape[0])])\n    class_feats = F.normalize(class_feats, dim=-1)\n    class_sim = new_test_feats @ class_feats.t()\n    weighted_class_sim = class_sim * (1 + alpha1 * torch.sum(new_test_feats ** 2, dim=1, keepdim=True))\n\n    # compute cache logits with feature channel importance\n    train_labels = F.one_hot(train_labels.view(-1))\n    key_logits = (new_train_feats @ new_clip_weights.t()).softmax(1)\n    cache_div = torch.sum(train_labels * torch.log2((train_labels + 1e-6) / (key_logits + 1e-6)), dim=1)[:, None]\n    soft_train_labels = train_labels * (cache_div * alpha2).exp()\n    R_fF = new_test_feats @ new_train_feats.t()\n    cache_logits = ((-1) * (alpha1 - alpha1 * R_fF)).exp() @ soft_train_labels\n\n    # fused logits with feature channel importance\n    logits = clip_logits + alpha0 * weighted_class_sim + alpha2 * cache_logits\n    \n    return logits",
          "objective": 38.72584,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes logits by combining zero-shot classification scores, a class-specific feature aggregation with feature-channel importance, and a feature-channel importance-weighted cache model, while introducing a feature-channel importance-based reweighting mechanism for the zero-shot logits.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    # feature selection\n    train_feats = train_feats.view(-1, train_feats.shape[-1])\n    new_clip_weights = F.normalize(clip_weights[:, indices], dim=-1)\n    new_train_feats = F.normalize(train_feats[:, indices], dim=-1)\n    new_test_feats = F.normalize(test_feats[:, indices], dim=-1)\n\n    # compute zero-shot logits with feature-channel importance reweighting\n    clip_logits = 100 * test_feats @ clip_weights.t()\n    channel_importance = torch.sum(new_test_feats ** 2, dim=1, keepdim=True)\n    clip_logits = clip_logits * (1 + alpha0 * channel_importance)\n\n    # compute class-specific feature aggregation with feature channel importance\n    class_feats = torch.stack([new_train_feats[train_labels.view(-1) == i].mean(dim=0) for i in range(clip_weights.shape[0])])\n    class_feats = F.normalize(class_feats, dim=-1)\n    class_sim = new_test_feats @ class_feats.t()\n    weighted_class_sim = class_sim * (1 + alpha1 * channel_importance)\n\n    # compute cache logits with feature channel importance\n    train_labels = F.one_hot(train_labels.view(-1))\n    key_logits = (new_train_feats @ new_clip_weights.t()).softmax(1)\n    cache_div = torch.sum(train_labels * torch.log2((train_labels + 1e-6) / (key_logits + 1e-6)), dim=1)[:, None]\n    soft_train_labels = train_labels * (cache_div * alpha2).exp()\n    R_fF = new_test_feats @ new_train_feats.t()\n    cache_logits = ((-1) * (alpha1 - alpha1 * R_fF)).exp() @ soft_train_labels\n\n    # fused logits with feature channel importance\n    logits = clip_logits + alpha1 * weighted_class_sim + alpha2 * cache_logits\n    \n    return logits",
          "objective": 38.72859,
          "other_inf": null
     }
]