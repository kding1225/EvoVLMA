[
     {
          "algorithm": "The new algorithm computes classification logits by combining zero-shot classifier logits, a feature channel importance-based logits with adaptive reweighting, a class prototype-based logits with dynamic scaling, and a weighted similarity-based cache model logits with adaptive kernel, while introducing a novel feature channel importance-based regularization term and a class-specific weighting scheme based on train labels.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    c, k, d = train_feats.shape\n    train_feats = train_feats.view(-1, d)\n    train_labels = F.one_hot(train_labels.view(-1)).type(train_feats.dtype)\n    \n    # Zero-shot classifier logits\n    clip_logits = 100 * test_feats @ clip_weights.t()\n    \n    # Feature channel importance-based logits with adaptive reweighting\n    important_feats = test_feats[:, indices]\n    important_weights = clip_weights[:, indices]\n    channel_importance = torch.norm(important_weights, dim=1, keepdim=True)\n    adaptive_weight = torch.mean(test_feats @ train_feats.t(), dim=1).unsqueeze(1)\n    channel_logits = 100 * (important_feats @ important_weights.t()) * (channel_importance.t() * adaptive_weight)\n    \n    # Class prototype-based logits with dynamic scaling\n    class_prototypes = train_feats.view(c, k, d).mean(dim=1)\n    prototype_scaling = 1 / (1 + alpha2 * torch.norm(class_prototypes, dim=1, keepdim=True))\n    prototype_logits = 100 * (test_feats @ class_prototypes.t()) * prototype_scaling.t()\n    \n    # Cache model logits with adaptive kernel\n    similarity_matrix = torch.exp(-alpha1 * (1 - test_feats @ train_feats.t()))\n    cache_logits = similarity_matrix @ train_labels\n    \n    # Feature channel importance-based regularization term\n    regularization_term = torch.norm(test_feats[:, indices], p=2, dim=1).unsqueeze(1) * torch.norm(clip_weights[:, indices], p=2, dim=1)\n    \n    # Class-specific weighting based on train labels\n    class_weights = train_labels.sum(dim=0).unsqueeze(0)\n    class_weighted_logits = cache_logits * class_weights\n    \n    # Combine all logits\n    logits = clip_logits + alpha0 * class_weighted_logits + alpha1 * channel_logits + alpha2 * prototype_logits + regularization_term\n    \n    return logits",
          "objective": 40.73599,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes classification logits by combining zero-shot classifier logits, a feature channel importance-based logits with adaptive reweighting, a class prototype-based logits with dynamic scaling, and a weighted similarity-based cache model logits with adaptive kernel, while introducing a novel feature channel importance-based regularization term and a class-specific weighting scheme based on train labels, and further enhancing the logits by incorporating a feature channel importance-based adaptive kernel for the cache model.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    c, k, d = train_feats.shape\n    train_feats = train_feats.view(-1, d)\n    train_labels = F.one_hot(train_labels.view(-1)).type(train_feats.dtype)\n    \n    # Zero-shot classifier logits\n    clip_logits = 100 * test_feats @ clip_weights.t()\n    \n    # Feature channel importance-based logits with adaptive reweighting\n    important_feats = test_feats[:, indices]\n    important_weights = clip_weights[:, indices]\n    channel_importance = torch.norm(important_weights, dim=1, keepdim=True)\n    adaptive_weight = torch.mean(test_feats @ train_feats.t(), dim=1).unsqueeze(1)\n    channel_logits = 100 * (important_feats @ important_weights.t()) * (channel_importance.t() * adaptive_weight)\n    \n    # Class prototype-based logits with dynamic scaling\n    class_prototypes = train_feats.view(c, k, d).mean(dim=1)\n    prototype_scaling = 1 / (1 + alpha2 * torch.norm(class_prototypes, dim=1, keepdim=True))\n    prototype_logits = 100 * (test_feats @ class_prototypes.t()) * prototype_scaling.t()\n    \n    # Cache model logits with adaptive kernel and feature channel importance\n    important_train_feats = train_feats[:, indices]\n    important_test_feats = test_feats[:, indices]\n    similarity_matrix = torch.exp(-alpha1 * (1 - important_test_feats @ important_train_feats.t()))\n    cache_logits = similarity_matrix @ train_labels\n    \n    # Feature channel importance-based regularization term\n    regularization_term = torch.norm(test_feats[:, indices], p=2, dim=1).unsqueeze(1) * torch.norm(clip_weights[:, indices], p=2, dim=1)\n    \n    # Class-specific weighting based on train labels\n    class_weights = train_labels.sum(dim=0).unsqueeze(0)\n    class_weighted_logits = cache_logits * class_weights\n    \n    # Combine all logits\n    logits = clip_logits + alpha0 * class_weighted_logits + alpha1 * channel_logits + alpha2 * prototype_logits + regularization_term\n    \n    return logits",
          "objective": 40.74397,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes classification logits by combining zero-shot classifier logits, a feature channel importance-based logits with adaptive reweighting, a class prototype-based logits with dynamic scaling, a weighted similarity-based cache model logits with adaptive kernel, and a novel feature channel importance-based regularization term, while introducing a class-specific weighting scheme based on train labels and a dynamic feature channel importance adjustment.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    c, k, d = train_feats.shape\n    train_feats = train_feats.view(-1, d)\n    train_labels = F.one_hot(train_labels.view(-1)).type(train_feats.dtype)\n    \n    # Zero-shot classifier logits\n    clip_logits = 100 * test_feats @ clip_weights.t()\n    \n    # Feature channel importance-based logits with adaptive reweighting\n    important_feats = test_feats[:, indices]\n    important_weights = clip_weights[:, indices]\n    channel_importance = torch.norm(important_weights, dim=1, keepdim=True)\n    adaptive_weight = torch.mean(test_feats @ train_feats.t(), dim=1).unsqueeze(1)\n    channel_logits = 100 * (important_feats @ important_weights.t()) * (channel_importance.t() * adaptive_weight)\n    \n    # Class prototype-based logits with dynamic scaling\n    class_prototypes = train_feats.view(c, k, d).mean(dim=1)\n    prototype_scaling = 1 / (1 + alpha2 * torch.norm(class_prototypes, dim=1, keepdim=True))\n    prototype_logits = 100 * (test_feats @ class_prototypes.t()) * prototype_scaling.t()\n    \n    # Cache model logits with adaptive kernel\n    similarity_matrix = torch.exp(-alpha1 * (1 - test_feats @ train_feats.t()))\n    cache_logits = similarity_matrix @ train_labels\n    \n    # Feature channel importance-based regularization term\n    regularization_term = torch.norm(test_feats[:, indices], p=2, dim=1).unsqueeze(1) * torch.norm(clip_weights[:, indices], p=2, dim=1)\n    \n    # Class-specific weighting based on train labels\n    class_weights = train_labels.sum(dim=0).unsqueeze(0)\n    class_weighted_logits = cache_logits * class_weights\n    \n    # Dynamic feature channel importance adjustment\n    dynamic_channel_importance = torch.mean(test_feats[:, indices] @ important_weights.t(), dim=1).unsqueeze(1)\n    dynamic_channel_logits = 100 * (important_feats @ important_weights.t()) * dynamic_channel_importance\n    \n    # Combine all logits\n    logits = clip_logits + alpha0 * class_weighted_logits + alpha1 * channel_logits + alpha2 * prototype_logits + regularization_term + dynamic_channel_logits\n    \n    return logits",
          "objective": 40.75555,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes classification logits by combining zero-shot classifier logits, a feature channel importance-based logits with adaptive reweighting, a class prototype-based logits with adaptive scaling, and a weighted similarity-based cache model logits with adaptive kernel, while introducing a novel feature channel importance-based reweighting mechanism and a class prototype-based logits with adaptive scaling, and further enhancing the logits by incorporating a class-specific weighting scheme based on the train labels, and adding a feature channel importance-based adaptive kernel for the cache model.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    c, k, d = train_feats.shape\n    train_feats = train_feats.view(-1, d)\n    train_labels = F.one_hot(train_labels.view(-1)).type(train_feats.dtype)\n    \n    # Zero-shot classifier logits\n    clip_logits = 100 * test_feats @ clip_weights.t()\n    \n    # Feature channel importance-based logits with adaptive reweighting\n    important_feats = test_feats[:, indices]\n    important_weights = clip_weights[:, indices]\n    channel_importance = torch.norm(important_weights, dim=1, keepdim=True)\n    channel_logits = 100 * (important_feats @ important_weights.t()) * channel_importance.t()\n    \n    # Class prototype-based logits with adaptive scaling\n    class_prototypes = train_feats.view(c, k, d).mean(dim=1)\n    prototype_scaling = 1 / (1 + alpha2 * torch.norm(class_prototypes, dim=1, keepdim=True))\n    prototype_logits = 100 * (test_feats @ class_prototypes.t()) * prototype_scaling.t()\n    \n    # Cache model logits with adaptive kernel and feature channel importance\n    important_train_feats = train_feats[:, indices]\n    important_test_feats = test_feats[:, indices]\n    similarity_matrix = torch.exp(-alpha1 * (1 - important_test_feats @ important_train_feats.t()))\n    cache_logits = similarity_matrix @ train_labels\n    \n    # Class-specific weighting based on train labels\n    class_weights = train_labels.sum(dim=0).unsqueeze(0)\n    class_weighted_logits = cache_logits * class_weights\n    \n    # Combine all logits\n    logits = clip_logits + alpha0 * class_weighted_logits + alpha1 * channel_logits + alpha2 * prototype_logits\n    \n    return logits",
          "objective": 40.76635,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes classification logits by combining zero-shot classifier logits, a feature channel importance-based logits with adaptive reweighting, a class prototype-based logits with dynamic scaling, a weighted similarity-based cache model logits with adaptive kernel, a novel feature channel importance-based regularization term, and a class-specific weighting scheme based on train labels, while introducing a dynamic feature channel importance adjustment and a class-specific feature channel importance enhancement.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    c, k, d = train_feats.shape\n    train_feats = train_feats.view(-1, d)\n    train_labels = F.one_hot(train_labels.view(-1)).type(train_feats.dtype)\n    \n    # Zero-shot classifier logits\n    clip_logits = 100 * test_feats @ clip_weights.t()\n    \n    # Feature channel importance-based logits with adaptive reweighting\n    important_feats = test_feats[:, indices]\n    important_weights = clip_weights[:, indices]\n    channel_importance = torch.norm(important_weights, dim=1, keepdim=True)\n    adaptive_weight = torch.mean(test_feats @ train_feats.t(), dim=1).unsqueeze(1)\n    channel_logits = 100 * (important_feats @ important_weights.t()) * (channel_importance.t() * adaptive_weight)\n    \n    # Class prototype-based logits with dynamic scaling\n    class_prototypes = train_feats.view(c, k, d).mean(dim=1)\n    prototype_scaling = 1 / (1 + alpha2 * torch.norm(class_prototypes, dim=1, keepdim=True))\n    prototype_logits = 100 * (test_feats @ class_prototypes.t()) * prototype_scaling.t()\n    \n    # Cache model logits with adaptive kernel\n    similarity_matrix = torch.exp(-alpha1 * (1 - test_feats @ train_feats.t()))\n    cache_logits = similarity_matrix @ train_labels\n    \n    # Feature channel importance-based regularization term\n    regularization_term = torch.norm(test_feats[:, indices], p=2, dim=1).unsqueeze(1) * torch.norm(clip_weights[:, indices], p=2, dim=1)\n    \n    # Class-specific weighting based on train labels\n    class_weights = train_labels.sum(dim=0).unsqueeze(0)\n    class_weighted_logits = cache_logits * class_weights\n    \n    # Dynamic feature channel importance adjustment\n    dynamic_channel_importance = torch.mean(test_feats[:, indices] @ important_weights.t(), dim=1).unsqueeze(1)\n    dynamic_channel_logits = 100 * (important_feats @ important_weights.t()) * dynamic_channel_importance\n    \n    # Class-specific feature channel importance enhancement\n    class_specific_importance = torch.mean(train_feats.view(c, k, d)[:, :, indices], dim=1)\n    class_specific_logits = 100 * (important_feats @ class_specific_importance.t())\n    \n    # Combine all logits\n    logits = clip_logits + alpha0 * class_weighted_logits + alpha1 * channel_logits + alpha2 * prototype_logits + regularization_term + dynamic_channel_logits + class_specific_logits\n    \n    return logits",
          "objective": 40.77967,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes classification logits by combining zero-shot classifier logits, a feature channel importance-based logits with adaptive reweighting, a class prototype-based logits with dynamic scaling, a weighted similarity-based cache model logits with adaptive kernel, and a novel feature channel importance-based regularization term, while introducing a class-specific weighting scheme based on train labels and a dynamic feature channel importance adjustment, and further enhancing the logits by incorporating a feature channel importance-based adaptive kernel for the cache model.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    c, k, d = train_feats.shape\n    train_feats = train_feats.view(-1, d)\n    train_labels = F.one_hot(train_labels.view(-1)).type(train_feats.dtype)\n    \n    # Zero-shot classifier logits\n    clip_logits = 100 * test_feats @ clip_weights.t()\n    \n    # Feature channel importance-based logits with adaptive reweighting\n    important_feats = test_feats[:, indices]\n    important_weights = clip_weights[:, indices]\n    channel_importance = torch.norm(important_weights, dim=1, keepdim=True)\n    adaptive_weight = torch.mean(test_feats @ train_feats.t(), dim=1).unsqueeze(1)\n    channel_logits = 100 * (important_feats @ important_weights.t()) * (channel_importance.t() * adaptive_weight)\n    \n    # Class prototype-based logits with dynamic scaling\n    class_prototypes = train_feats.view(c, k, d).mean(dim=1)\n    prototype_scaling = 1 / (1 + alpha2 * torch.norm(class_prototypes, dim=1, keepdim=True))\n    prototype_logits = 100 * (test_feats @ class_prototypes.t()) * prototype_scaling.t()\n    \n    # Cache model logits with adaptive kernel and feature channel importance\n    important_train_feats = train_feats[:, indices]\n    important_test_feats = test_feats[:, indices]\n    similarity_matrix = torch.exp(-alpha1 * (1 - important_test_feats @ important_train_feats.t()))\n    cache_logits = similarity_matrix @ train_labels\n    \n    # Feature channel importance-based regularization term\n    regularization_term = torch.norm(test_feats[:, indices], p=2, dim=1).unsqueeze(1) * torch.norm(clip_weights[:, indices], p=2, dim=1)\n    \n    # Class-specific weighting based on train labels\n    class_weights = train_labels.sum(dim=0).unsqueeze(0)\n    class_weighted_logits = cache_logits * class_weights\n    \n    # Dynamic feature channel importance adjustment\n    dynamic_channel_importance = torch.mean(test_feats[:, indices] @ important_weights.t(), dim=1).unsqueeze(1)\n    dynamic_channel_logits = 100 * (important_feats @ important_weights.t()) * dynamic_channel_importance\n    \n    # Combine all logits\n    logits = clip_logits + alpha0 * class_weighted_logits + alpha1 * channel_logits + alpha2 * prototype_logits + regularization_term + dynamic_channel_logits\n    \n    return logits",
          "objective": 40.78188,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes classification logits by combining zero-shot classifier logits, a feature channel importance-based logits with adaptive reweighting, a class prototype-based logits with dynamic scaling, a weighted similarity-based cache model logits with adaptive kernel, a novel feature channel importance-based regularization term, and a dynamic feature channel importance adjustment, while introducing a class-specific weighting scheme based on train labels and a feature channel importance-based dynamic scaling.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    c, k, d = train_feats.shape\n    train_feats = train_feats.view(-1, d)\n    train_labels = F.one_hot(train_labels.view(-1)).type(train_feats.dtype)\n    \n    # Zero-shot classifier logits\n    clip_logits = 100 * test_feats @ clip_weights.t()\n    \n    # Feature channel importance-based logits with adaptive reweighting\n    important_feats = test_feats[:, indices]\n    important_weights = clip_weights[:, indices]\n    channel_importance = torch.norm(important_weights, dim=1, keepdim=True)\n    adaptive_weight = torch.mean(test_feats @ train_feats.t(), dim=1).unsqueeze(1)\n    channel_logits = 100 * (important_feats @ important_weights.t()) * (channel_importance.t() * adaptive_weight)\n    \n    # Class prototype-based logits with dynamic scaling\n    class_prototypes = train_feats.view(c, k, d).mean(dim=1)\n    prototype_scaling = 1 / (1 + alpha2 * torch.norm(class_prototypes, dim=1, keepdim=True))\n    prototype_logits = 100 * (test_feats @ class_prototypes.t()) * prototype_scaling.t()\n    \n    # Cache model logits with adaptive kernel\n    similarity_matrix = torch.exp(-alpha1 * (1 - test_feats @ train_feats.t()))\n    cache_logits = similarity_matrix @ train_labels\n    \n    # Feature channel importance-based regularization term\n    regularization_term = torch.norm(test_feats[:, indices], p=2, dim=1).unsqueeze(1) * torch.norm(clip_weights[:, indices], p=2, dim=1)\n    \n    # Class-specific weighting based on train labels\n    class_weights = train_labels.sum(dim=0).unsqueeze(0)\n    class_weighted_logits = cache_logits * class_weights\n    \n    # Dynamic feature channel importance adjustment\n    dynamic_channel_importance = torch.mean(test_feats[:, indices] @ important_weights.t(), dim=1).unsqueeze(1)\n    dynamic_channel_logits = 100 * (important_feats @ important_weights.t()) * dynamic_channel_importance\n    \n    # Feature channel importance-based dynamic scaling\n    dynamic_scaling = 1 / (1 + alpha0 * torch.norm(test_feats[:, indices], p=2, dim=1).unsqueeze(1))\n    dynamic_scaled_logits = dynamic_channel_logits * dynamic_scaling\n    \n    # Combine all logits\n    logits = clip_logits + alpha0 * class_weighted_logits + alpha1 * channel_logits + alpha2 * prototype_logits + regularization_term + dynamic_scaled_logits\n    \n    return logits",
          "objective": 40.78799,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes classification logits by combining zero-shot classifier logits, a feature channel importance-based logits with adaptive gating, a class prototype-based logits with dynamic scaling, and a weighted similarity-based cache model logits with adaptive kernel, while introducing a novel feature channel importance-based gating mechanism and a class prototype-based logits with dynamic scaling.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    c, k, d = train_feats.shape\n    train_feats = train_feats.view(-1, d)\n    train_labels = F.one_hot(train_labels.view(-1)).type(train_feats.dtype)\n    \n    # Zero-shot classifier logits\n    clip_logits = 100 * test_feats @ clip_weights.t()\n    \n    # Feature channel importance-based logits with adaptive gating\n    important_feats = test_feats[:, indices]\n    important_weights = clip_weights[:, indices]\n    channel_importance = torch.norm(important_weights, dim=1, keepdim=True)\n    gating_weights = torch.sigmoid(torch.mean(test_feats @ train_feats.t(), dim=1).unsqueeze(1))\n    channel_logits = 100 * (important_feats @ important_weights.t()) * (channel_importance.t() * gating_weights)\n    \n    # Class prototype-based logits with dynamic scaling\n    class_prototypes = train_feats.view(c, k, d).mean(dim=1)\n    prototype_scaling = 1 / (1 + alpha2 * torch.norm(class_prototypes, dim=1, keepdim=True))\n    prototype_logits = 100 * (test_feats @ class_prototypes.t()) * prototype_scaling.t()\n    \n    # Cache model logits with adaptive kernel\n    similarity_matrix = torch.exp(-alpha1 * (1 - test_feats @ train_feats.t()))\n    cache_logits = similarity_matrix @ train_labels\n    \n    # Combine all logits\n    logits = clip_logits + alpha0 * cache_logits + alpha1 * channel_logits + alpha2 * prototype_logits\n    \n    return logits",
          "objective": 40.80601,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes classification logits by combining zero-shot classifier logits, a feature channel importance-based logits with adaptive attention, a class prototype-based logits with dynamic scaling, and a weighted similarity-based cache model logits with adaptive kernel, while introducing a novel feature channel importance-based attention mechanism and a class prototype-based logits with dynamic scaling, and further enhances the logits by incorporating a feature channel importance-based regularization term.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    c, k, d = train_feats.shape\n    train_feats = train_feats.view(-1, d)\n    train_labels = F.one_hot(train_labels.view(-1)).type(train_feats.dtype)\n    \n    # Zero-shot classifier logits\n    clip_logits = 100 * test_feats @ clip_weights.t()\n    \n    # Feature channel importance-based logits with adaptive attention\n    important_feats = test_feats[:, indices]\n    important_weights = clip_weights[:, indices]\n    channel_importance = torch.norm(important_weights, dim=1, keepdim=True)\n    attention_weights = torch.softmax(channel_importance, dim=0)\n    channel_logits = 100 * (important_feats @ important_weights.t()) * attention_weights.t()\n    \n    # Class prototype-based logits with dynamic scaling\n    class_prototypes = train_feats.view(c, k, d).mean(dim=1)\n    prototype_scaling = 1 / (1 + alpha2 * torch.norm(class_prototypes, dim=1, keepdim=True))\n    prototype_logits = 100 * (test_feats @ class_prototypes.t()) * prototype_scaling.t()\n    \n    # Cache model logits with adaptive kernel\n    similarity_matrix = torch.exp(-alpha1 * (1 - test_feats @ train_feats.t()))\n    cache_logits = similarity_matrix @ train_labels\n    \n    # Feature channel importance-based regularization\n    regularization = torch.norm(test_feats[:, indices], dim=1, keepdim=True)\n    \n    # Combine all logits\n    logits = clip_logits + alpha0 * cache_logits + alpha1 * channel_logits + alpha2 * prototype_logits + regularization\n    \n    return logits",
          "objective": 40.8241,
          "other_inf": null
     },
     {
          "algorithm": "The new algorithm computes classification logits by combining zero-shot classifier logits, a feature channel importance-based logits with adaptive attention, a class prototype-based logits with dynamic scaling, and a weighted similarity-based cache model logits with adaptive kernel, while introducing a novel feature channel importance-based attention mechanism and a class prototype-based dynamic scaling, and further enhancing the logits with a feature channel importance-based regularization term.",
          "code": "import torch\nimport torch.nn.functional as F\n\ndef compute_logits(train_feats, train_labels, test_feats, clip_weights, indices, alpha0, alpha1, alpha2):\n    c, k, d = train_feats.shape\n    train_feats = train_feats.view(-1, d)\n    train_labels = F.one_hot(train_labels.view(-1)).type(train_feats.dtype)\n    \n    # Zero-shot classifier logits\n    clip_logits = 100 * test_feats @ clip_weights.t()\n    \n    # Feature channel importance-based logits with adaptive attention\n    important_feats = test_feats[:, indices]\n    important_weights = clip_weights[:, indices]\n    channel_importance = torch.norm(important_weights, dim=1, keepdim=True)\n    attention_weights = torch.softmax(channel_importance, dim=0)\n    attention_logits = 100 * (important_feats @ (attention_weights * important_weights).t())\n    \n    # Class prototype-based logits with dynamic scaling\n    class_prototypes = train_feats.view(c, k, d).mean(dim=1)\n    prototype_scaling = 1 / (1 + alpha2 * torch.norm(class_prototypes, dim=1, keepdim=True))\n    prototype_logits = 100 * (test_feats @ class_prototypes.t()) * prototype_scaling.t()\n    \n    # Cache model logits with adaptive kernel\n    similarity_matrix = torch.exp(-alpha1 * (1 - test_feats @ train_feats.t()))\n    cache_logits = similarity_matrix @ train_labels\n    \n    # Feature channel importance-based regularization\n    regularization = torch.norm(test_feats[:, indices], dim=1, keepdim=True)\n    \n    # Combine all logits\n    logits = clip_logits + alpha0 * cache_logits + alpha1 * attention_logits + alpha2 * prototype_logits + regularization\n    \n    return logits",
          "objective": 40.8254,
          "other_inf": null
     }
]